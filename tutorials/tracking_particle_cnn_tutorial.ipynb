{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepTrack 2.0 - Tracking a point particle with a CNN\n",
    "\n",
    "This tutorial demonstrates how to track a point particle with a convolutional neural network (CNN) using DeepTrack 2.0.\n",
    "\n",
    "Specifically, this tutotial explains how to: \n",
    "* Define the procedure to generate training images\n",
    "* Extract information from these images to use as labels for the training\n",
    "* Define and train a neural network model\n",
    "* Visually evaluate the quality of the neural network output\n",
    "\n",
    "It is recommended to peruse this tutotial after the [deeptrack_introduction_tutorial](deeptrack_introduction_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Setup\n",
    "\n",
    "Imports needed for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.scatterers import PointParticle\n",
    "from deeptrack.optics import Fluorescence\n",
    "from deeptrack.generators import Generator\n",
    "from deeptrack.models import convolutional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Define the particle\n",
    "\n",
    "For this example, we consider a point particle (a point light scatterer). A point particle is an instance of the class `PointParticle` (see also [scatterers_example](../examples/scatterers_example.ipynb)), whose properties are controlled by the following parameters:\n",
    "\n",
    "* `intensity`: The intensity of the point particle\n",
    "\n",
    "* `position`: The position of the point particle\n",
    "\n",
    "* `position_unit`: \"pixel\" or \"meter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_particle = PointParticle(                                         \n",
    "    intensity=100,\n",
    "    position=(32, 16),\n",
    "    position_unit=\"pixel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Define the optical system \n",
    "\n",
    "Next, we need to define the properties of the optical system. This is done using an instance of the class `Optics` (see also [optics_example](../examples/optics_example.ipynb)), which takes a set of particles (light scatterers) and convolves them with the pupil function (point spread function) of the optical system. In this tutorial, there is only one light scatterer (here, `point_particle`).\n",
    "\n",
    "The optics is controlled by the following parameters:\n",
    "\n",
    "* `NA`: The numerical aperature\n",
    "\n",
    "* `resolution`: The effective camera pixel size (m)\n",
    "\n",
    "* `magnification`: The magnification of the optical device\n",
    "\n",
    "* `wavelength`: The wavelength of the lightsource (m)\n",
    "\n",
    "* `output_region`: The position of the camera and the number of pixels (x, y, width_x, width_y)\n",
    "\n",
    "* `upscale`: upscale factor for the pupil function (increases accuracy and computational cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "\n",
    "fluorescence_microscope = Fluorescence(\n",
    "    NA=0.7,                \n",
    "    resolution=1e-6,     \n",
    "    magnification=10,\n",
    "    wavelength=680e-9,\n",
    "    output_region=(0, 0, IMAGE_SIZE, IMAGE_SIZE),\n",
    "    upscale=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Create and plot the image\n",
    "\n",
    "To view some object through an optical device, we call the optical device (here, `fluorescence_microscope`) with the object we want to image (here, `point_particle`). This creates a new object (here, `imaged_particle`) that can be used to generate the desired image.\n",
    "\n",
    "The image is finally generated by calling `imaged_particle.resolve()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaged_particle = fluorescence_microscope(point_particle)\n",
    "\n",
    "output_image = imaged_particle.resolve()\n",
    "\n",
    "plt.imshow(np.squeeze(output_image), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Randomize the particle position\n",
    "\n",
    "We can generate particles with random positions by passing to the keyword argument `position` a lambda function that returns a pair of random numbers representing the particle position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate particle with random position\n",
    "\n",
    "particle_with_random_position = PointParticle(                                         \n",
    "    intensity=100,\n",
    "    position=lambda: np.random.rand(2) * IMAGE_SIZE,\n",
    "    position_unit=\"pixel\"\n",
    ")\n",
    "\n",
    "imaged_particle_with_random_position = fluorescence_microscope(particle_with_random_position)\n",
    "\n",
    "output_image = imaged_particle_with_random_position.resolve()\n",
    "\n",
    "plt.imshow(np.squeeze(output_image), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The position can then be retrieved from the attribute `.position` of the generated image. `.properties` contains a list of all properties used to create the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve particle position\n",
    "\n",
    "def get_position_of_particle(image):\n",
    "    for image_property in image.properties:\n",
    "        if \"position\" in image_property:\n",
    "            return image_property[\"position\"]\n",
    "\n",
    "position_of_particle = get_position_of_particle(output_image)\n",
    "\n",
    "plt.imshow(np.squeeze(output_image), cmap='gray')\n",
    "plt.scatter(position_of_particle[1], position_of_particle[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Define the neural network model\n",
    "\n",
    "We will use a predefined neural network model obtained by calling the function `convolutional` (see also [models_example](../examples/models_example.ipynb)). This model is a convolutional neural network with a dense top. It receives as input an image of shape `(64, 64, 1)` and outputs two scalar values corresponing to the x and y position of the particle.\n",
    "\n",
    "The model can be customized using the following arguments\n",
    "\n",
    "* `input_shape`: Size of the images to be analyzed.\n",
    "\n",
    "* `conv_layers_dimensions`: Number of convolutions in each convolutional layer.\n",
    "    \n",
    "* `dense_layers_dimensions`: Number of units in each dense layer.\n",
    "        \n",
    "* `number_of_outputs`: Number of units in the output layer.\n",
    "\n",
    "* `output_activation`: The activation function applied to the output layer.\n",
    "\n",
    "* `loss`: The loss function of the network.\n",
    "\n",
    "* `optimizer`: The the optimizer used for training.\n",
    "\n",
    "* `metrics`: Additional metrics to evaulate during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convolutional(\n",
    "    input_shape=(64, 64, 1), \n",
    "    number_of_outputs=2\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Define image generator\n",
    "\n",
    "Generators are objects that feed models with images and their corresponging labels during training. They are created by calling `.generate()` on an instance of the class `Generator` (see also [generators_example](../examples/generators_example.ipynb)). This method takes the following inputs:\n",
    "* `feature`: A feature (see also [features_example](../examples/features_example.ipynb) that resolves images used to train a model (here, `imaged_particle_with_random_position`)\n",
    "* `label_function`: A function that takes an image as input and returns the label for that image (here, `get_position_of_particle`)\n",
    "* `batch_size`: The number of images per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that retireves the position of a particle \n",
    "# and divides it by 64 to get values between 0 and 1\n",
    "def get_scaled_position_of_particle(image):\n",
    "    position_of_particle = get_position_of_particle(image)\n",
    "    return position_of_particle / 64\n",
    "\n",
    "generator = Generator().generate(\n",
    "    imaged_particle_with_random_position, \n",
    "    get_scaled_position_of_particle, \n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Train the model\n",
    "\n",
    "The model is trained by calling the method `.fit()` with the generator we defined in the previous step. Be patient, this might take some time (several minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    generator,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Visualize the model performance\n",
    "\n",
    "We can now use the trained model to measure the particle position in images previously unseen by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, real_positions = next(generator)\n",
    "\n",
    "measured_positions = model.predict(images)\n",
    "\n",
    "for i in range(images.shape[0]):\n",
    "    \n",
    "    image = np.squeeze(images[i])\n",
    "    \n",
    "    measured_position_x = measured_positions[i, 1] * IMAGE_SIZE\n",
    "    measured_position_y = measured_positions[i, 0] * IMAGE_SIZE\n",
    "\n",
    "    real_position_x = real_positions[i, 1] * IMAGE_SIZE\n",
    "    real_position_y = real_positions[i, 0] * IMAGE_SIZE\n",
    "\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.scatter(real_position_x, real_position_y, s=70, c='r', marker='x')\n",
    "    plt.scatter(measured_position_x, measured_position_y, s=100, marker='o', facecolor='none', edgecolors='b')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}