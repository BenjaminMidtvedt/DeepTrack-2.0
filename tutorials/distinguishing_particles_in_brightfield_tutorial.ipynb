{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepTrack 2.0 - Distinguishing particles in brightfield microscopy\n",
    "\n",
    "This tutorial demonstrates how to use a U-net to track and distinguish particles of different sizes in brightfield microscopy.\n",
    "\n",
    "This tutorial should be read after the tutorials [deeptrack_introduction_tutorial](deeptrack_introduction_tutorial.ipynb) and [tracking_multiple_particles_unet_tutorial](tracking_multiple_particles_unet_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Setup\n",
    "\n",
    "Imports needed for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.scatterers import Sphere\n",
    "from deeptrack.optics import Brightfield, IlluminationGradient\n",
    "from deeptrack.noises import Poisson\n",
    "from deeptrack.generators import Generator\n",
    "from deeptrack.models import unet\n",
    "from deeptrack.aberrations import Zernike\n",
    "from deeptrack.augmentations import FlipLR, FlipUD, FlipDiagonal\n",
    "from deeptrack.math import NormalizeMinMax\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Define the particles\n",
    "\n",
    "We consider spherical particles of two different sizes. The particles are instances of the class `Sphere`, which takes the following inputs:\n",
    "\n",
    "* `position`: The lateral position of the particle\n",
    "* `z` (optional): The axial distance of the particle perpendicualr to the focal plane\n",
    "* `position_unit`: \"pixel\" or \"meter\", defines the scale factor of the particle position\n",
    "* `radius`: The radius of the particle in meters\n",
    "* `refractive_index`: The refractive index of the particle\n",
    "\n",
    "In this example, the positions of the particles are randomly sampled with a lateral position between 0 and 256 pixels in the focal plane, and an axial position between -10 to 10 pixels from the focal plane. The smaller particle has a radius between 200 and 250 nm, while the larger has a radius between 400 and 500 nm. The real part of their refractive index is 1.45, while the imaginary part is between 0.1 and 0.15 (corresponding to the absorption coefficient). Finally we define a dummy-property, `particle_type`, which helps us distinguish between the particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_particle = Sphere(\n",
    "    position=lambda: np.random.rand(2) * 256,\n",
    "    z=lambda: - 10 + np.random.rand() * 20,\n",
    "    position_unit=\"pixel\",\n",
    "    radius=lambda: 200e-9 + np.random.rand() * 50e-9,\n",
    "    refractive_index=lambda: 1.45 + (0.1j + np.random.rand() * 0.05j),\n",
    "    particle_type = 0\n",
    ")\n",
    "\n",
    "large_particle = Sphere(\n",
    "    position=lambda: np.random.rand(2) * 256,\n",
    "    z=lambda: -10 + np.random.rand() * 20,\n",
    "    position_unit=\"pixel\",\n",
    "    radius=lambda: 400e-9 + np.random.rand() * 100e-9,\n",
    "    refractive_index=lambda: 1.45 + (0.1j + np.random.rand() * 0.05j),\n",
    "    particle_type = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Define the optics \n",
    "\n",
    "Next, we need to define the properties of the optical system. Here, we use the class `Brightfield`, which uses particles with a defined refractive_index and illuminates them coherently (see also [optics_example](../examples/optics_example.ipynb)). It takes the following inputs:\n",
    "\n",
    "* `wavelength`: The wavelength of the illuminting light (m)\n",
    "* `NA`: The numerical aperature\n",
    "* `resolution`: The effective camera pixel size (m)\n",
    "* `magnification`: The magnification of the optical device\n",
    "* `output_region`: The position and size of the camera sensor in pixels (x, y, width_x, width_y)\n",
    "* `refractive_index_medium`: The refractive index of the medium the scatterers are immersed in. \n",
    "* `illumination`: The light illuminating to the sample (if undefined, the sample is illuminated homogenously with intensity 1)\n",
    "\n",
    "To simulate incoherent white light, we define multiple optical devices in a range of wavelengths from 400 to 700 nm. The number of optical devices is a trade-off between accuracy and speed: in most cases a few (~3) wavelengths will be enough. Moreover, we create an instance of the class `IlluminationGradient`, which adds an intensity gradient to the illuminating light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum = np.linspace(400e-9, 700e-9, 5)\n",
    "\n",
    "illumination_gradient = IlluminationGradient(gradient=lambda: np.random.randn(2) * 0.0002)\n",
    "\n",
    "brightfield_microscope = [Brightfield(\n",
    "                            wavelength=wavelength,\n",
    "                            NA=0.95,\n",
    "                            resolution=1e-6,\n",
    "                            magnification=10,\n",
    "                            refractive_index_medium=1.33,\n",
    "                            illumination=illumination_gradient,\n",
    "                            output_region=(0, 0, 256, 256))\n",
    "                          for wavelength \n",
    "                          in spectrum]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Define noises\n",
    "\n",
    "The noise in the system is Poisson distributed with SNR between 50 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = Poisson(snr=lambda: 50 + np.random.rand() * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Combine the features \n",
    "\n",
    "The sample contains 5 to 24 small particles, and 5 to 24 large particles. It is illuminated with the brightfield optics defined above. We use the python function `sum()` to add the optical devices together. This is equivalent to the more verbose statement `optics[0](sample) + optics[1](sample) + ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_small_particles = lambda: np.random.randint(5, 25)\n",
    "number_of_large_particles = lambda: np.random.randint(5, 25)\n",
    "\n",
    "sample = small_particle**number_of_small_particles + large_particle**number_of_large_particles\n",
    "\n",
    "incoherently_illuminated_sample = sum([brightfield_microscope_one_wavelegth(sample) \n",
    "                                       for brightfield_microscope_one_wavelegth \n",
    "                                       in brightfield_microscope])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Augmenting the images\n",
    "\n",
    "Simulating many images is slow. to speed up the training, we use augmentation. These are special features that allow us to resolve more images before resolving the base feature (`brightfield_microscope_one_wavelegth`). We use 3 augmentations, `FlipLR`, which mirrors the image left to right, `FlipUD`, which mirrors the image up to down, and `FlipDiagonal`, which mirrors the image along the main diagonal. This results in an 8-fold increase in the numebr of images, and thus in the speed of the code (roughly).\n",
    "\n",
    "We add the noise after the augmentation as a cheap way to make the images even more diverse.\n",
    "\n",
    "Finally we normalize the images using `NormalizeMinMax`, which transforms the images to values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_image = FlipUD(FlipLR(FlipDiagonal(incoherently_illuminated_sample)))\n",
    "\n",
    "image_of_particles = augmented_image + noise + NormalizeMinMax()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Plot example images\n",
    "\n",
    "Now, we visualize some example images. At each iteration, we call the method `.update()` to refresh the random features in the image (particle number, particle position, Poisson noise, etc.). Afterwards we call the method `.plot()` to generate and display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    image_of_particles.update()\n",
    "    image_of_particles.plot(cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Create the target images\n",
    "\n",
    "We define a function that uses the generated images to create the target images to be used in the training. The target is an image of shape (256, 256, 3), where the last dimension represents three classes. In other words, each pixel in the input image is classified into one of three classes:\n",
    "1. The first class is the null-class, representing the background. A pixel belongs to the null class if it doesn't belong to any other class. \n",
    "2. The second class is 1 if the pixel is within 3 pixels of a small particle.\n",
    "3. The third class is 1 if the pixel is within 3 pixels of a large particle.\n",
    "\n",
    "We also show images and targets side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_image(image_of_particles):\n",
    "    label = np.zeros((*image_of_particles.shape[:2], 3))\n",
    "    \n",
    "    X, Y = np.meshgrid(\n",
    "        np.arange(0, image_of_particles.shape[0]), \n",
    "        np.arange(0, image_of_particles.shape[1])\n",
    "    )\n",
    "\n",
    "    for property in image_of_particles.properties:\n",
    "        if \"position\" in property:\n",
    "            position = property[\"position\"]\n",
    "            distance_map = (X - position[1])**2 + (Y - position[0])**2\n",
    "            label[distance_map < 9, property[\"particle_type\"] + 1] = 1\n",
    "            \n",
    "    label[..., 0] = 1 - np.max(label[..., 1:], axis=-1)\n",
    "    \n",
    "    return label\n",
    "\n",
    "# Show images\n",
    "for i in range(4):\n",
    "    augmented_image.update()\n",
    "    \n",
    "    image_of_particles = augmented_image.resolve()\n",
    "    \n",
    "    label_of_particles = get_target_image(image_of_particles)\n",
    "    \n",
    "    plt.figure(figsize=(12, 9))\n",
    "    \n",
    "    plt.subplot(1, 4,1)\n",
    "    plt.imshow(image_of_particles[..., 0], cmap=\"gray\")\n",
    "    \n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(label_of_particles[..., 0], cmap=\"gray\")\n",
    "    \n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(label_of_particles[..., 1], cmap=\"gray\")\n",
    "    \n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(label_of_particles[..., 2], cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Define image generator\n",
    "\n",
    "We define a generator that creates images and targets in batches of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().generate(\n",
    "    augmented_image, \n",
    "    get_target_image,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Define loss function\n",
    "\n",
    "We also define a custom loss function. The loss function is categorical loss, where each class is wighted by 1-p, where p is is the proportion of all pixels in that class which is 1 in the label. This helps us avoid the obvious local minimum of classifying all pixels as the null-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "def softmax_categorical(T, P):\n",
    "    classwise_weight = K.mean(1 - T, axis=(1, 2), keepdims=True)\n",
    "    true_error = K.mean(T * K.log(P + eps) * classwise_weight, axis=-1)\n",
    "    return -K.mean(true_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Define the neural network model\n",
    "\n",
    "The neural network architecture used is a U-net, which is a fully convoltional model used for image to image transformations (see also [models_example](../examples/models_example.ipynb)). We add a softmax activation to the final layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet(\n",
    "    (256, 256, 1), \n",
    "    conv_layers_dimensions=[8, 16, 32],\n",
    "    base_conv_layers_dimensions=[32, 32], \n",
    "    number_of_outputs=3,\n",
    "    output_activation=\"softmax\",\n",
    "    loss=softmax_categorical\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Train the model\n",
    "\n",
    "We train the model by calling `.fit()`. This step will take some time (about 1 hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(generator,\n",
    "          epochs=100,\n",
    "          steps_per_epoch=10,\n",
    "          max_queue_size=0,\n",
    "          workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Visualize the model performance\n",
    "\n",
    "Finally, we evaluate the model performance by showing the model output beside the input image and the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().generate(\n",
    "    augmented_image, \n",
    "    get_target_image,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "input_image, target_image = next(generator)\n",
    "\n",
    "predicted_image = model.predict(input_image)\n",
    "\n",
    "for i in range(input_image.shape[0]):\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 11), constrained_layout=True)\n",
    "\n",
    "    gs = fig.add_gridspec(2, 5)\n",
    "\n",
    "    fig.add_subplot(gs[0:2, 0:2])\n",
    "    plt.imshow(input_image[i, :, :, 0], cmap=\"gray\")\n",
    "    plt.title(\"Input Image\")\n",
    "\n",
    "    fig.add_subplot(gs[0, 2])\n",
    "    plt.imshow(predicted_image[i, :, :, 0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(\"Predicted Image \\n null class\")\n",
    "    \n",
    "    fig.add_subplot(gs[0, 3])\n",
    "    plt.imshow(predicted_image[i, :, :, 1], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(\"Predicted Image \\n small particles\")\n",
    "    \n",
    "    fig.add_subplot(gs[0, 4])\n",
    "    plt.imshow(predicted_image[i, :, :, 2], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(\"Predicted Image \\n large particles\")\n",
    "\n",
    "    \n",
    "    fig.add_subplot(gs[1, 2])\n",
    "    plt.imshow(target_image[i, :, :, 0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(\"Target Image \\n null class\")\n",
    "    \n",
    "    fig.add_subplot(gs[1, 3])\n",
    "    plt.imshow(target_image[i, :, :, 1], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(\"Target Image \\n small particles\")\n",
    "    \n",
    "    fig.add_subplot(gs[1, 4])\n",
    "    plt.imshow(target_image[i, :, :, 2], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(\"Target Image \\n large particles\")  \n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}