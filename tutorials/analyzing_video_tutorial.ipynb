{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing videos of a particle\n",
    "\n",
    "In this tutorial, we will train a network to extract the intensity of a particle regardless of focus. The network will be trained to predict on videos of centered particles moving in and out of focus randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Setup\n",
    "\n",
    "Imports needed for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.scatterers import Sphere\n",
    "from deeptrack.optics import Fluorescence\n",
    "from deeptrack.noises import Offset, Poisson\n",
    "from deeptrack.aberrations import SphericalAberration, HorizontalComa, VerticalComa\n",
    "from deeptrack.augmentations import Augmentation, FlipLR, FlipUD, FlipDiagonal\n",
    "from deeptrack.sequences import Sequence, Sequential\n",
    "from deeptrack.features import Feature\n",
    "from deeptrack.generators import Generator\n",
    "from deeptrack.models import RNN\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define the particle\n",
    "\n",
    "We want to generate a video where the position of the particle changes over time. The x-y position of the particle will be roughly centered in each frame. This is to simulate a tracked particle centered by cropping a region of interest around it for each frame. The z position, however, will follow a Brownian motion with a random diffusion constant. \n",
    "\n",
    "To allow these properties to change in the video, we define two functions that return the value of these properties for each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position():\n",
    "    return 32 + np.random.randn(2) * 0.5\n",
    "\n",
    "def get_z(previous_value, diffusion_constant, dt=1/30):\n",
    "    return np.clip(previous_value + np.random.randn() * np.sqrt(diffusion_constant * dt) * 1e7, -60, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Set initial values of the properties\n",
    "\n",
    "The properties passed to create the instance of `Sphere` are used for the first frame of the video.\n",
    "\n",
    "* `position` is initialized with the same function as that used for each subsequent frame.\n",
    "* `z` is initialized as a normally distributed number centered around zero.\n",
    "* `intensity` is constant 1. The intensity will be varied in a later step.\n",
    "* `radius` is a random number between 200nm and 1000nm\n",
    "* `position_unit` is set to pixels\n",
    "* `upsample` is 4, increasing the resolution of the sphere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_feature = Sphere(\n",
    "    position=get_position,\n",
    "    z=lambda: np.random.randn() * 20,\n",
    "    intensity=1,\n",
    "    radius=lambda: 200e-9  + np.random.rand()*800e-9,\n",
    "    diffusion_constant=lambda: (1 + np.random.rand() * 9) * 1e-12,\n",
    "    position_unit=\"pixel\",\n",
    "    upsample=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Make it sequential\n",
    "\n",
    "To tell `particle_feature` how to change for each frame in a video, we call `Sequential` with the two functions defined in 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_particle = Sequential(particle_feature, z=get_z, position=get_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define the full feature series\n",
    "\n",
    "We define an optical device with some aberration. It is hard for the network to learn with heavy aberration. To mitigate this, the network will start training with very little aberration, and slowly increase the aberration as the network trains. \n",
    "\n",
    "To achieve this, we define a dummy property, `epoch`, which will return a number which increases by one each time the property is updated, up to `200`. The property `coefficient`, in turn, returns a normally distributed random number, with a standard deviation that scales with `epoch`. This way, the more times the property is updated, the more the optical device is aberrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_cap = 200\n",
    "get_coefficient = lambda epoch: np.random.randn() * 0.1 * epoch / epoch_cap\n",
    "\n",
    "spherical_aberration = SphericalAberration(epoch=iter(range(1, epoch_cap + 1)), \n",
    "                                           coefficient=get_coefficient)\n",
    "\n",
    "horizontal_coma = HorizontalComa(epoch=iter(range(1, epoch_cap + 1)), \n",
    "                                 coefficient=get_coefficient)\n",
    "\n",
    "vertical_coma = VerticalComa(epoch=iter(range(1, epoch_cap + 1)), \n",
    "                             coefficient=get_coefficient)\n",
    "\n",
    "aberrations = spherical_aberration + horizontal_coma + vertical_coma\n",
    "\n",
    "optics = Fluorescence(\n",
    "    NA=0.7,\n",
    "    magnification=10,\n",
    "    resolution=(1e-6, 1e-6, 1e-6),\n",
    "    wavelength=633e-9,\n",
    "    output_region=(0, 0, 64, 64),\n",
    "    pupil=aberrations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The on-camera captured intensity scales with the radius cubed times the property `intensity`. This means that the apparent intensity is much more dependent on the radius of the object than the intensity of each voxel. To remedy this, we define and apply a normalization feature to the `Sphere`. `NormalizeSum` ensure that the sum of all pixels in the input is equal to `scale`. Thus, the apparent intensity will only depend on the property `intensity` of the sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeSum(Feature):\n",
    "    def get(self, image, scale=1, **kwargs):\n",
    "        return image / np.sum(image) * scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Bring it together\n",
    "\n",
    "We define the full feature series by imaging the sequential particle and the normalization feature with `optics`. `Sequence(imaged_article, sequence_length=10)`, in turn, creates a feature that resolves videos of length 10.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaged_particle = optics(sequential_particle + NormalizeSum(scale=100)) \n",
    "\n",
    "imaged_particle_sequence = Sequence(imaged_particle, sequence_length=10)  \n",
    "\n",
    "imaged_particle_sequence.update()\n",
    "imaged_particle_sequence.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Vary the intensity\n",
    "\n",
    "Varying the intensity of the particle is equivalent to scaling the output image. This allows us to define an augmentation that converts a video of a particle of any intensity to any other intensity.\n",
    "\n",
    "We additionally define the method `update_properties`. This method is unique to the class `Augmentation`, and it lets us update the properties of the video in accordance with the augmentation used. Here, it updates all instances of the `intensity` property in the video by multiplying it by the `multiplier` property of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentIntensity(Augmentation):\n",
    "    def get(self, image, multiplier, number_of_updates, **kwargs):\n",
    "        return image * multiplier\n",
    "    \n",
    "    def update_properties(self, image, multiplier, **kwargs):\n",
    "        for prop in image.properties:\n",
    "            if \"intensity\" in prop:\n",
    "                prop[\"intensity\"] *= multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Augment the videos\n",
    "\n",
    "For augmentation we will use a 8-way mirroring augmentation, followed by the aforementioned intensity augmentation. Noise is added after the augmentation to further diversify the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_particle_sequence = FlipUD(FlipLR(FlipDiagonal(imaged_particle_sequence)))\n",
    "augmented_particle_sequence = AugmentIntensity(augmented_particle_sequence, \n",
    "                                               multiplier=lambda: 1 + np.random.rand() * 9,\n",
    "                                               updates_per_reload=2)\n",
    "\n",
    "augmented_particle_sequence += Offset(offset=0.1) + Poisson(snr=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_particle_sequence.update()\n",
    "augmented_particle_sequence.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Define the model\n",
    "\n",
    "The model used will be `RNN`, which is a Recurrent Neural Network. The model acts like a convolutional neural network applied to each step of the video, followed by a LSTM layer, which merges all the information into a single output. RNNs can model complicated temporal behavours of videos of arbitrary length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_shape=(None, 64, 64, 1), \n",
    "            rnn_layers_dimensions=(32, 32),\n",
    "            dense_layers_dimensions=(32, 32),\n",
    "            number_of_outputs=1, \n",
    "            loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Define the label and the generator\n",
    "\n",
    "The label function extracts the property `intensity` from the first frame of the video. The generator is a standard generator with `ndim` set to 5 to account for it resolving videos instead of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intensity_of_particle(video):\n",
    "    return video[0].get_property(\"intensity\")\n",
    "\n",
    "generator = Generator().generate(\n",
    "    augmented_particle_sequence, \n",
    "    get_intensity_of_particle, \n",
    "    batch_size=4,\n",
    "    ndim=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the model\n",
    "\n",
    "We train the model for 30 epochs, consisting of 100 batches eachs. This may take some time (upwards of half an hour).\n",
    "\n",
    "Note that some versions of Tensorflow do not correctly handle videos like these correctly. If you see an error such as `GPU sync failed` or any error pointing to there being too little memory, consider trying Tensorflow version 1.14, or tf-nightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(generator, epochs=200, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validate the model\n",
    "\n",
    "We validate the correctness of the model by generating 200 videos and creating a scatter plot of predicted intensity vs. actual intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().generate(\n",
    "    augmented_particle_sequence, \n",
    "    get_intensity_of_particle, \n",
    "    batch_size=1,\n",
    "    ndim=5\n",
    ")\n",
    "\n",
    "predicted_intensity = []\n",
    "true_intensity = []\n",
    "for _ in range(200):\n",
    "    batch, labels = next(generator)\n",
    "    batch = np.asarray(batch)\n",
    "\n",
    "    prediction = model.predict(batch)\n",
    "    predicted_intensity.append(prediction[0])\n",
    "    true_intensity.append(labels[0])\n",
    "        \n",
    "plt.scatter(true_intensity, predicted_intensity)\n",
    "plt.plot([1, 10], [1, 10])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title(\"Predicted intensity vs true intensity\")\n",
    "plt.xlabel(\"True intensity\")\n",
    "plt.ylabel(\"Predicted intensity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}