{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepTrack 2.0 - Tracking multiple particles with a U-net\n",
    "\n",
    "This tutorial demonstrates how to track multiple particles using a U-net with DeepTrack 2.0.\n",
    "\n",
    "The U-net receives as input an image that may or may not contain particles and outputs an image whose pixels represent the probability that there is a particle nearby. Specifically, each pixel has a value between 0 (high confidence that there is no particle close by) and 1 (high confidence that there is a nearby particle).\n",
    "\n",
    "This tutorial should be perused after the tutorials [deeptrack_introduction_tutorial](deeptrack_introduction_tutorial.ipynb) and [tracking_particle_cnn_tutorial](tracking_particle_cnn_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Imports needed for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.scatterers import PointParticle\n",
    "from deeptrack.optics import Fluorescence\n",
    "from deeptrack.noises import Poisson, Offset\n",
    "from deeptrack.generators import Generator\n",
    "from deeptrack.models import unet\n",
    "from deeptrack.losses import weighted_crossentropy, sigmoid, flatten\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the particle\n",
    "\n",
    "For this example, we consider point particles (point light scatterers). A point particle is an instance of the class `PointParticle`, defined by its intensity and its position. Here, the position is randomized using a lambda function. More details can be found in the tutorial [tracking_particle_cnn_tutorial](tracking_particle_cnn_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle = PointParticle(                                         \n",
    "    intensity=100,\n",
    "    position=lambda: np.random.rand(2) * 256, \n",
    "    position_unit=\"pixel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the optical system \n",
    "\n",
    "Next, we need to define the properties of the optical system. This is done using an instance of the class `Fluorescence`, which takes a set of light scatterers (particles) and convolves them with the pupil function (point spread function) of the optical system. More details can be found in the tutorial [tracking_particle_cnn_tutorial](tracking_particle_cnn_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluorescence_microscope = Fluorescence(\n",
    "    NA=0.7,                \n",
    "    resolution=1e-6,     \n",
    "    magnification=10,\n",
    "    wavelength=680e-9,\n",
    "    output_region=(0, 0, 256, 256)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define noises\n",
    "\n",
    "We introduce two sources of noise (see also [noises_example](../examples/noises_example.ipynb)):\n",
    "1. A background random offset between 0 and 1.\n",
    "2. A Poisson noise with a random SNR between 20 and 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = Offset(\n",
    "    offset=lambda: np.random.rand()*1\n",
    ")\n",
    "\n",
    "poisson_noise = Poisson(\n",
    "    snr=lambda: np.random.rand()*30 + 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the image features\n",
    "\n",
    "We want images with a random number of particles between 1 and 10, a background offset, and Poisson noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = lambda: np.random.randint(1, 11)\n",
    "\n",
    "image_features = fluorescence_microscope(particle**num_particles) + offset + poisson_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot example images\n",
    "\n",
    "Now, we visualize some example images. At each iteration, we call the method `.update()` to refresh the random features in the image (particle number, particle positions, offset level, and Poisson noise). Afterwards we call the method `.plot()` to generate and display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    image_features.update()\n",
    "    output_image = image_features.plot(cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create the target images\n",
    "\n",
    "We define a function that uses the generated images to create the target images to be used in the training. Here the target image is binary image, where each pixel is `1` if it is within `CIRCLE_RADIUS` distance from any particle in the input image, and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an image with circles of radius two at the same position \n",
    "# as the particles in the input image.\n",
    "\n",
    "CIRCLE_RADIUS = 3\n",
    "\n",
    "def get_target_image(image_of_particles):\n",
    "    target_image = np.zeros(image_of_particles.shape)\n",
    "    X, Y = np.meshgrid(\n",
    "        np.arange(0, image_of_particles.shape[0]), \n",
    "        np.arange(0, image_of_particles.shape[1])\n",
    "    )\n",
    "\n",
    "    for property in image_of_particles.properties:\n",
    "        if \"position\" in property:\n",
    "            position = property[\"position\"]\n",
    "\n",
    "            distance_map = (X - position[1])**2 + (Y - position[0])**2\n",
    "            target_image[distance_map < CIRCLE_RADIUS**2] = 1\n",
    "    \n",
    "    return target_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we show images and targets side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4d7b2b83db46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mimage_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mimage_of_particles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtarget_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_target_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_of_particles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_features' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    image_features.update()\n",
    "    image_of_particles = image_features.resolve()\n",
    "\n",
    "    target_image = get_target_image(image_of_particles)\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(np.squeeze(image_of_particles), cmap=\"gray\")\n",
    "    plt.title(\"Input Image\")\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(np.squeeze(target_image), cmap=\"gray\")\n",
    "    plt.title(\"Target image\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define image generator\n",
    "\n",
    "We define a generator that creates images and targets in batches of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().generate(\n",
    "    image_features, \n",
    "    get_target_image,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define the neural network model\n",
    "\n",
    "The neural network architecture used is a U-Net, which is a fully convolutional model used for image-to-image transformations. We create this model by calling the function `unet` (see also [models_example](../examples/models_example.ipynb)).\n",
    "\n",
    "Since the desired output is a binary image, we will be using crossentropy as loss. Furthermore, since the target image is disproportionaly populated by 0s (any pixel is much more likely to be a zero than a one), we weight the loss such that false negatives are penalized ten times more than the false positives. \n",
    "\n",
    "The model can be customized by passing the following arguments:\n",
    "\n",
    "* `input_shape`: Size of the images to be analyzed. The first two values can be set to `None` to allow arbitrary sizes.\n",
    "\n",
    "* `conv_layers_dimensions`: Number of convolutions in each convolutional layer during down-\n",
    "    and upsampling.\n",
    "    \n",
    "* `base_conv_layers_dimensions`: Number of convolutions in each convolutional layer at the base\n",
    "    of the unet, where the image is the most downsampled.\n",
    "\n",
    "* `output_conv_layers_dimensions`: Number of convolutions in each convolutional layer after the\n",
    "    upsampling.\n",
    "    \n",
    "* `steps_per_pooling`: Number of convolutional layers between each pooling and upsampling\n",
    "    step.\n",
    "\n",
    "* `number_of_outputs`: Number of convolutions in output layer.\n",
    "\n",
    "* `output_activation`: The activation function of the output.\n",
    "\n",
    "* `loss`: The loss function of the network.\n",
    "\n",
    "* `optimizer`: The the optimizer used for training.\n",
    "\n",
    "* `metrics`: Additional metrics to evaulate during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet(\n",
    "    (256, 256, 1), \n",
    "    conv_layers_dimensions=[8, 16, 32],\n",
    "    base_conv_layers_dimensions=[32, 32], \n",
    "    loss=flatten(weighted_crossentropy((10, 1)))\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train the model\n",
    "\n",
    "The model is trained by calling `.fit()`. This will take some time on the order of 10s of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    generator, \n",
    "    epochs=50,          \n",
    "    steps_per_epoch=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize the model performance\n",
    "\n",
    "Finally, we evaluate the model performance by showing the model output besides the input image and the target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image, target_image = next(generator)\n",
    "\n",
    "for i in range(input_image.shape[0]):\n",
    "    \n",
    "    predicted_image = model.predict(input_image)\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(np.squeeze(input_image[i, :, :, 0]), cmap=\"gray\")\n",
    "    plt.title(\"Input Image\")\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(np.squeeze(predicted_image[i, :, :, 0]), cmap=\"gray\")\n",
    "    plt.title(\"Predicted Image\")\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(np.squeeze(target_image[i, :, :, 0] > 0.5), cmap=\"gray\")\n",
    "    plt.title(\"Target image\")\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
